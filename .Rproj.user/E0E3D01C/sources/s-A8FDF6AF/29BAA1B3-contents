---
title: "Exercise 4"
---

#Step 2.Description of data
In step 1 I created a new chapter called chapter 4. The MASS library contains the Boston data set, which has 506 obs. of  14 variables. The 506 observations are records (housing values) for 506 neighborhoods (suburbs) around Boston.The following 14 variables are included in Boston:


 * crim   : num  per capita crime rate by town.
 * zn     : num  proportion of residential land zoned for lots over 25,000 sq.ft
 * indus  : num  proportion of non-retail business acres per town.
 * chas   : int  Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
 * nox    : num  nitrogen oxides concentration (parts per 10 million).
 * rm     : num  average number of rooms per dwelling.
 * age    : num  proportion of owner-occupied units built prior to 1940..
 * dis    : num  weighted mean of distances to five Boston employment centres.
 * rad    : int  index of accessibility to radial highways..
 * tax    : num  full-value property-tax rate per \$10,000.
 * ptratio: num  pupil-teacher ratio by town..
 * black  : num  1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
 * lstat  : num  lower status of the population (percent)..
 * medv   : num  median value of owner-occupied homes in \$1000s.


```{r message=F, warnings=F}
##Load the Boston data from the MASS package
library(MASS)
data(Boston)
##structure of Boston data
str(Boston)
##dimensions of Boston data
dim(Boston)
```


#Step 3

##Numerical summary of data
The summary of the variables in the data is shown below. Based on the large differences between mean and median of values of "black", "tax", "rad","age", "indus", "zn" and "crim", looks like the distribution of these are skewed.

```{r}

library(MASS);library(knitr)
(summary(Boston))
```
#Graphical overview of the distributions
Figure 1 shows a graphical summary of the distribution of all variables in the dataset.Based on that:

** Age is skewed and there is proportionally more older people in the dataset.
** The distribution of the proprtion of non-retail business (indus) and the tax rates (tax) are bimodial.
**

```{r warning=F, message=F, fig.height=9, fig.width=9}
library(ggplot2);library(dplyr);library(tidyr)
gather(Boston) %>% ggplot() + geom_density(aes(value)) + facet_wrap("key", scales = "free")+theme_minimal()+ggtitle("Figure 1. Distributions of variables in Boston data") 
```

##Relationships between variables in the data
Figure 2 shows the relationships between variables in Boston data, by looking at the correlations between the data. Tax is strongly (r2 above 0.7) positively correlated with rad and indus. Overall, the data set seems to be quite correlated.

```{r warning=F, message=F, fig.height=9, fig.width=9}
library(tidyverse);library(MASS);library(corrplot)
# MASS, corrplot, tidyverse and Boston dataset are available

# calculate the correlation matrix 
cor_matrix<-cor(Boston) 

# visualize the correlation matrix
corrplot(cor_matrix, method="color",  
     diag=FALSE,
     type="upper", 
     order="hclust", 
     title="Figure 2. Correlations between variables in Boston data",
     addCoef.col = "black", # Add coefficient of correlation
     mar=c(0,0,1,0) 
     )



```


#Step 4 
Whole dataset Boston was Standardized using scale(), summaries of the scaled data are shown below. The means of the variables are now 0.

Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)


```{r echo=FALSE}
library(MASS);library(dplyr)
# MASS and Boston dataset are available

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

#make quantiles of crim rate in scaled boston data
bins <- quantile(boston_scaled$crim)
#check values of bin
bins

#based on the quatnile values made the new variable crim
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)
#variable looks good, quite equal group sizes, 4 quantiles
table(crime)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))
table(crime)

#remove crim (not crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
#new variable crime added to dataset
boston_scaled <- data.frame(boston_scaled, crime)

#Dividing the data set into train (80% of data) and test
n <- nrow(boston_scaled)
#randomly selected indivials 80%
ind <- sample(n,  size = n * 0.8)
#training set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]


```

Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot. (0-3 points)

Linear discriminant analysis:
target variable is categorical and this classification method tries to find the variables (or finds the (linear) combination of the variables) that disciminate/sepratate the classes/groups in the target variable best.But it can be also used to predict classes in the new data.
Assumptions: variables are normally distributed, variables have same variance and that is the reasons variables were scaled previously

```{r fig.height=9, fig.width=9}
#lda of trained data
lda.fit <- lda(crime~ ., data = train)
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```


Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)
```{r}

library(MASS)
#classe from 
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.fit <- lda(crime~ ., data = train)
lda.pred <- predict(lda.fit, newdata = test)


table(correct = correct_classes, predicted = lda.pred$class)

```

Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)
```{r fig.height=9, fig.width=9}
library(MASS);library(ggplot2)
data('Boston')
boston_scaled2 <- as.data.frame(scale(Boston))

dist_eu <-dist(boston_scaled2)
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)


# k-means clustering
km <-kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)


```

